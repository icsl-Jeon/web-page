import Image from "next/image";
import YouTube, { YouTubeProps } from "react-youtube";
import ImageGrid from "@/components/ImageGrid";
import { Card } from "@/components/PostSummary";
import Accordion from "@/components/Accordian";
import { targets, skills, hardware } from "../public/data";

<div>
<Image
  src={"/images/my-photo.png"}
  width={300}
  height={300}
  className="mx-auto rounded-full my-5"
/>
<p className="text-center text-2xl my-3 font-bold">Felipe Jeon</p>
<p className="text-center">

  ü§ñ I build a robot  autonomy combining design, perception, reasoning, planning, and control. 
</p>
<p className="text-center">
üé® I build generative diffusion-based large vision  AI model.  
</p>
</div>
<br />

<div className="mx-auto flex flex-row items-center justify-center mb-6">
  <a href="https://drive.google.com/file/d/1MVzmoGdF_g8l-1qdM9iFvNLW1su9z4O8/view?usp=sharing" target="_blank">
  <button
    className={
      "shadow  bg-slate-200 hover:bg-slate-300 text-center text-black p-1 rounded-lg mx-1 px-4"
    }
  >
    cv
  </button>
  </a>
  <a href="https://github.com/icsl-Jeon" target="_blank">
  <button
  className={
    "shadow  bg-slate-200 hover:bg-slate-300 text-center p-1 text-black rounded-lg  mx-1  px-4"
  }
  >
    Github
  </button>
  </a>
 <a href="https://www.linkedin.com/in/felipe-jeon-491773226/" target="_blank">
  <button
  className={
    "shadow  bg-slate-200 hover:bg-slate-300 text-center p-1 text-black rounded-lg  mx-1  px-4"
  }
  >
    LinkedIn
  </button>
  </a>
   <a href="https://www.youtube.com/channel/UCPeLtCD0ouhFdLO60V7pjlw" target="_blank">

  <button
  className={
    "shadow  bg-slate-200 hover:bg-slate-300 text-center p-1 text-black rounded-lg  mx-1  px-4"
  }
  >
    Youtube
  </button>
  </a>
</div>


## ü§ñ Robotics
<br />


<Accordion title="üì¶ Deliverables" isInitialOpen={false}>
<div className="p-3">
### 1. Motion (major) ü§∏ 


<div className="sm:px-5 my-10">
  <YouTube videoId="ZBFRS5TpRvI" className="videoWrapper" />
</div>


#### A. Tasks
I focused on generating optimal motions for the below tasks:
- Visible motion to chase moving targets (Ph. D research topic, [git](https://github.com/icsl-Jeon/dual_chaser), [paper](https://ieeexplore.ieee.org/abstract/document/9196703))
- Motion for enhancing color detectability ([paper](https://ieeexplore.ieee.org/abstract/document/9341368/))
- Safe travel from A to B 
- Distributed motion and task allocation 
- Exploration in unknown environments   
- Inverse kinematics of manipulators ([paper](https://ieeexplore.ieee.org/abstract/document/8287457))

#### B. Hardware Targets
The motions were targeted and tested to the below robot types:
<div className="p-2">
  <ImageGrid col1 = {5} col2={5} data={targets} linkOn={false}></ImageGrid>
</div>

#### C. Backgrounds
- Search / sampling-based planning
- Spline motion primitives such as B-spline, Bezier, piecewise polynomials. ([git](https://github.com/icsl-Jeon/traj_gen))
- Non-holonomic curves (Dubins, Reeds-Shepp, continuous curvature)
- Optimization-based planning (iLQR, iLQG, DDP, CHOMP)
- Reinforcement Learning (DDPG, PPO) 


### 2. Perception üëÄ
To implement the motion autonomy in the real-world, I had multiple hands-on experiences in perception algorithms for traversability and localization.  
<div className="sm:px-5 my-10">
  <YouTube videoId="Ux1_LinLegM" className="videoWrapper" />
</div>

#### A. Volumetric Mapping 
Comfortable with optimization and tuning for the below algorithms to make occupancy from from 3D sensing.
- Octomap and distance field for 3D environments ([git](https://github.com/icsl-Jeon/octomap_mapping/tree/kinetic-devel))
- Voxblox (TSDF, EDSF)
- 3D mesh generation and opengl render from pointcloud ([git](https://github.com/icsl-Jeon/see_with_you))

#### B. SLAM  
- VIO (vins-mono, ZEDfu) ([git](https://github.com/icsl-Jeon/vo_comparison))
- Graph-based SLAM (RTAB-Map) or Lidar SLAM (LOAM)
- Intrinsic or extrinsic calibration (Kalibr)

### 3. Reasoning üß†  
In addition to perception (occupancy, ego-localization), [reasoning about targets of interest](https://youtu.be/ZBFRS5TpRvI?t=284) is a must for the aerial chasing system.
For real-world experiments, most of the reasoning methods were tested on the Jetson onboard computer.

#### A. Detection

Hands-on code integration to detect targets from the vision of flying drones. The below algorithms were performed: 
- [2D / 3D bounding box from image streams](https://youtu.be/RE6pJ6QvqsA?t=77)
- [Human skeleton detection](https://youtu.be/Ux1_LinLegM?t=207)


#### B. Segmentation

Given RGB and depth streams, I have used pixel segmentation in RGB and extracted 3D points from depth information.

<div className="sm:px-5 my-10">
  <YouTube videoId="2BkRbw13rdo" className="videoWrapper" />
</div>

  

#### C. 3D position tracking and prediction
Beyond detection in a single frame, 3D positions of the targets are tracked and predicted to plan the chasing motion of drones. 
- **[3D Tracking](https://youtu.be/Ux1_LinLegM?t=154)**: combining classical approaches (color, kalman filtering), [I fine-tuned](https://youtu.be/Ux1_LinLegM?t=641) to stably identify the same object even against occlusion and deformation.
- **Prediction**: based on the tracking algorithm, performed [3d position prediction](https://youtu.be/Ux1_LinLegM?t=182) reflecting obstacles.


### 4. Design & Integration 

#### A. Mechanical Design
- **Part selection**: multiple experiences in [making multiple drones myself](https://youtu.be/s0mc5209IiY?t=141), where I deliberated on the selection of battery, actuator, control & computing board, etc.
- **Parts design and building**: Solidworks [design](https://drive.google.com/file/d/1AZcatZrFxSqTmmSoW5S2rwYxjKdrG2O0/view?usp=drive_link) and simulation. Getting my hands dirty with soldering and other hardware stuffs.

#### B. Software Integration
- **Project**: Design pattern, cmake structuring for large projects ([example template](https://github.com/icsl-Jeon/simple-cmake-package))
- **Packaging and deployment**: Qt for [experiment gui](https://youtu.be/9tbOiXRAKQ8?t=67), Web frontend ([link](https://diverse-group-generation.vercel.app/)) 
- **Simulation**: testing ROS / cmake project in *unreal engine* using blueprint ([my tutorial video](https://www.youtube.com/watch?v=izSMF3auLsY))

</div>
</Accordion>

<Accordion title="‚öîÔ∏è Skills" isInitialOpen={false}>
<div>
‚úÖYou can click the links on images for relevant codes or media.
<div className="p-2">
<ImageGrid data={skills} linkOn={true} border={true} ></ImageGrid>
</div>

### 1. Fundamentals
- **Mathematics**: linear algebra, lie algebra, numerical methods and optimizations (SQP, MIQP).
- **Robotics**: representation (SE(3), exponential coordinate), kinematics (velocity, adjoint matrix, Jacobian), dynamics (wrench).
- **Machine learning**: reinforcement learning (DQN, PPO, DDPG), vision learning (CNN, ViT).
- **Algorithms**: graph & tree search, dynamic programming

### 2. Software
- **Project Management**: git, docker, jira, notion, cmake
- **Robotics**: C++(14-20), eigen, ros 1/2, qpOASES, unreal engine
- **Machine Vision**: opencv, open3d, PIL, opengl, meshlab
- **Machine Learning**: pytorch, stable-baseline, einops
- **Web**: typescript, react, nextjs, django, vercel, SQL
- **Etc**: adobe software, solidworks. 

### 3. Hardware 
‚ù§Ô∏èThe below images show some of drones I made myself. 
<ImageGrid data={hardware} border></ImageGrid>

- **Sensor**: ZED 1/2, Bluefox, d435, d455, T265, Velodyne, Auster, IMU, ublox GPS
- **Actuator & ESC**: [T-motor](https://uav-en.tmotor.com/), [DJI](https://www-v1.dji.com/e310.html), [xing motor](https://shop.iflight.com/xing2-2207-4s-6s-fpv-motor-unibell-pro1464) 
- **Control**: Pixhawk

</div>
</Accordion>

<Accordion title="üéì Experiences" isInitialOpen={false}>
<div className="p-2">
### 1. Education & Company
- **2013-2015**: Started from architecture and architectural engineering @ [Seoul National University](https://en.snu.ac.kr/).  
- **2015-2017**: BS in mechanical & aerospace engineering @ Seoul National University
- **2017-2022**: PhD in Robotics in [Lab for Autonomous Robotics Research](https://larr.snu.ac.kr/), advisor:
  [H. Jin Kim](https://scholar.google.com/citations?user=TLQUwIMAAAAJ&hl=ko).
  <b>5 yrs graduation </b>
- **2022-Current**: Staff engineer @ Samsung Research, Robot Intelligence Team.

### 2. Projects 

#### Graduate School
- Autonomous driving in unstructured environments @ Korea Electronics Technology Institute (KETI)
- Multi-fleet exploration for rescue robots @ Korea Institute of Robotics and Technology Convergence (KIRO)

#### Personal
- Diverse group generation using genetic algorithm ([link](https://diverse-group-generation.vercel.app/)) 
- Polynomial trajectory generation with constraints ([link](https://github.com/icsl-Jeon/traj_gen))
- Inferring attention of a human toward ambient objects using body skeleton and head pose ([youtube](https://www.youtube.com/watch?v=2BkRbw13rdo)). 

</div>
</Accordion>

<Accordion title="üòÄ Personality & Thoughts" isInitialOpen={false}>
<div className="p-2">
- I love people and enjoy mingling ‚ù§Ô∏è (in general)
- [MBTI](https://www.16personalities.com/personality-types) is **ESTJ**. Love organizing and planning to solve meaningful problems.   
- Respect nerds and geeks obsessed with coding, but I am not that kind, and I do not even want to be like them. 
- **Focus more on why and what**. Sick and tired of purposeless work, studying and research. (e.g., writing a paper for making a paper, studying coding for a higher leet code score)
- I think Ph D. guys can be worse than a cleaning worker or a chef, unless their techs could reach and help others in the world.  
</div>
</Accordion>


## üé®  Generative AI 
<br />

<Accordion title="üì¶ Deliverables" isInitialOpen={false}>
<div className="p-3">
### 1. Diffusion Model Development ‚≠ê
I have deep understanding & industrial experiences in Diffusion Models.
For [Galaxy AI](https://www.youtube.com/watch?v=SGM4FFfz1fk), I am working on Inpainting / outpainting *on-device* model. 
The model is focused on clean removal without unwanted object generation along with shadow removal in a single model.
Please note that random object generations in object removal is a big problem üóØÔ∏è due to the nature of diffusion :(, and my job is to correct it.  

{/* TODO: add photos */}

I am comfortable with debugging the diffusion (more like DDPM) based models (SD1.5, SDXL) and low-based models such as SD3 and Flux.
Also, I very frequently modify [their pipes](https://huggingface.co/docs/diffusers/api/pipelines/overview).


#### Adaptation from Large Foundation Models for On-Device Applications
Our models are mainly oriented to on-device application with strict limitations on ROM, peak memory, NPU evaluations, even in the model sizes.
To make adaptable LoRA weights from LVMs, I focus on how to get a good LoRA (A, B) with minimum target layers, and did multiple experiments on ranks.


### 2. Improving Models  ‚úèÔ∏è

#### Textual Inversion (Prompt Optimization)
{/* TODO: add photos of segscale*/}
For commecial usages of mobile devices, I focused on **minimal prompts operation** for fast UX. 
As an expert on this task in my team, I precisely compressed the abstract user intentions (even they often do not know how to put into texts üòº) into embedding spaces of the textual prompts, by showing the models with a good examples.  
Also, I made a research how to apply two different intentions by spatially varying latent regions by modifying CFG. 

#### Model Reinforcement
{/* https://confluence.sec.samsung.net/display/GVAI/AlignProps */}
Feeding the high-quality data to make good models, and optimizing task prompts are not enough for a good service.
To align various user preferences, I have experiences apply reinforcement techinques such as [AlighProp](https://github.com/mihirp1998/AlignProp).


</div>
</Accordion>





